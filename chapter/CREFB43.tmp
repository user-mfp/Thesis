\chapter{Processes and Setups}
\label{installation}

In this chapter I will describe the system in detail. This involves the \textit{interaction paradigm}, the \textit{interface designs} and the \textit{technical principles} behind the system. 

%-----------------------------------------------------------------------------

%\textbf{How visitors become users. Spaces [Hornecker et al.], unmittelbare Reaktion, Idiogram mit Anweisung, FuÃŸspuren + Radius, Feedback der Zeigeposition auf Skizze des Grabes mit Exponaten}
\paragraph{Interaction} 
In a typical \textit{presentation software}'s scenario, one or more visitors can be involved. At the moment they enter the room of the showcase, the system recognizes them and changes its appearance. The screen will display a short and explanatory text about how the system should be used. In addition, an ideogram visualizes the description by showing a figure pointing at a plane in front of it. This view can be seen in Figure \ref{fig:intro}. If there are no visitors present, the screen will disguise itself by displaying an image of the showcases background. Because the system can only be used by one person at a time, there are footsteps on the floor (see Figure \ref{fig:footsteps}) to distinguish the user from other visitors. 
\\
In \cite{UrbanHCI}, Fischer at al. describe a variety of spaces around large, public, and interactive installations. I used an adapted approach to initiate interaction between visitors and our installation. Hence, the area in front of the showcase could be interpreted as an \textit{activation} or \textit{potential interaction space}, because at least the system will react to the visitors' presence and shows itself. This is the initialization of a possible interaction. Here, the \textit{interaction space} is predefined and thus there is no \textit{potential interaction space}~\cite{UrbanHCI}. Other spaces, such as \textit{gap} and \textit{social space} are also available (see Figure \ref{fig:uhci_hassleben}). 
\\
Once a user enters the interaction space, the system reacts again. This time, the screen will show the outlines of the exhibits inside the showcase. The position of each selectable exhibit is semi-transparently displayed over the outlines. Now, the user can point at exhibits of interest. The system will track the user's movements, and will calculate the position he or she is pointing at. This position is also overlayed on the outlines as shown in Figure \ref{fig:sketch}. To be distinguishable, exhibits' positions are  highlighted in blue, while the current pointing position is red. Users can utilize this visual feedback to correct their gesture in order to 'hit' their intended target.
\\
An exhibit gets selected, after the user constantly pointed at it for over half a second. This \textit{dwelltime} prevents unintended selections. Otherwise, any exhibit would be immediately selected, whenever a user's pointing position strokes over an exhibit. After selecting an exhibit, a corresponding description will be displayed alongside detailed images of the exhibit (see Figure \ref{fig:show}). While the text will stay, the images will be displayed as a slide show. Afterwards, the system will go back to the exhibition's overview again. The user does not have to end a slide show and can wait until it is over or select a new exhibit while it is still running - without visual feedback though.
\\
If a user is done or others want to try the interface out for themselves, users can change any time. They only have to switch places on the footsteps. The same applies for ending the session. -- Visitors can leave at any point during the interaction. As soon as the system does not recognize any visitors around the showcase anymore, it will go back into its disguised appearance again. Again, no additional input devices are needed except people's natural behavior.

The \textit{administration software} is more traditional and requires a keyboard and a mouse. Nevertheless, it also makes use of pointing gestures. This software can be used by one experienced user alone. But the ones that are less advanced might need a second person's assistance. The administration software can be used to create and edit \ac{IMI}-exhibitions. Therefore, all necessary data can be defined with it. This data is saved in \textit{configuration files} of XML-format. Those files are used by the presentation software and can be reloaded for editing by the administration software. Expert users can make changes with a text editor as well. 
\\
An exhibition consists of two main components. They are the \textit{exhibition plane} and the \textit{exhibits}. Both are defined by pointing. Upon the application's start, an existing exhibition can be loaded or a new one can be created by naming it. In case of a new exhibition, the first thing to be defined is the exhibition plane. Therefore, the administrative user again has the choice to either load an existing plane or define a new one. Should the user decide to define an exhibition plane, the procedure will be explained in a short instruction, before the calibration begins. It tells the user, to point at three of the planes corners from three different positions. Further, the user is told to follow the instructions during the process. In addition, ideograms like Figure \ref{fig:ideo_cali} depicting the positions to go to and the corners to point at are shown during the process. Once the process is started it will guide the user from position to position and corner to corner in fixed intervals between three and five seconds. After having pointed at the corners once, a second run is needed to validate the corners' positions. This is necessary, because a position could be defined wrong. If the validation is successful, the exhibition and its plane are saved. The second main component are the exhibits. They can only be defined or loaded, if a raw exhibition with a valid exhibition plane is available. The definition of an exhibit is similar to that of the exhibition plane. The user points at the respective exhibit on the earlier defined exhibition plane from three different positions. Afterwards, the position is validated.
\\
One more necessary setting is the prospective \textit{user position}. It is one of the exhibition's settings and can be set by simply choosing it from the drop-down menu and confirming with a button press. Subsequently, the administrative user gets the instruction to stand where future users are supposed to stand to be in the interaction space. After confirming and a short wait to get in position, the software will safe the position. Only visitors standing close to this spot will be able to use the system. When defining the user position, there should only be one person in the scope of the camera.
\\
The remaining parameters of both exhibition and exhibits have default values, which can be edited later. There are specially labeled buttons, which lead to the corresponding drop-down menus. Only the exhibition plane can not be changed this way.

%-----------------------------------------------------------------------------

%\textbf{Appearance. Administration and Presentation GUIs [Shneiderman], K.I.S.S.}
\paragraph{Interfaces} The presentation software has a minimal \ac{UI}. There are no control elements like buttons or menus. It only has the four different states shown in Figure \ref{fig:ui_pres}.
\\
The first one is the \textit{standby state}. It is active, if no person in front of the showcase is recognized. The corresponding view's task is to mask the system. Therefore, the screen will only show an image of the showcase's background in order to disguise itself against it. 
\\
The next state gets activated, if visitors are present and none of them is inside the dedicated interaction space. This \textit{introduction state}'s task is to make visitors aware of the systems presence and invite one of them to interact with the installation. As shown in Figure \ref{fig:ui_pres}b, only a short introduction and an ideogram are needed to explain how the system has to be used. In Order to seamlessly integrate into the general style of the museum, the font was chosen to be the museum's corporate font. Further, the background color was chosen to fit with the ambient color of the showcase, and to reduce the contrast between text and background. Moreover, the font size was increased to ensure readability. The associated calculations can be found in the next chapter. 
\\
The system gets into its third state as soon as a user inside the dedicated interaction space. It can be described as an \textit{overview state}. Figure \ref{fig:ui_pres}c shows the selectable exhibits' positions on top of all exhibits' outlines as blue ellipse and the user's pointing position as a red dot. While the exhibits' positions are always marked, the red \textit{feedback position} only appears, if a user is really pointing at that particular spot of the exhibition plane. Otherwise, the red dot will not be shown.
\\
The fourth state is the actual \textit{presentation state}. It is similar to the introduction, but here the text and images depend on which exhibit had been selected. The corresponding texts are limited to approximately 300 characters, so visitors do not have to read extensively long descriptions. Meanwhile, detailed images of the chosen exhibits are displayed on the right half of the screen (see Figure \ref{fig:ui_pres}d). They change every four seconds. This time an image shown for can be adjusted either by the administration software of by hand. After all images have been shown, the system will go back to any of the previous states. Where it goes depends on the visitors behavior. If a user is present in the interaction space, the system will go to the overview. If there are visitors present, but no one qualifies as a user, the introduction will be displayed. If everybody left during the presentation, the system will stand by and hide again.
%Presentation -> only three views: (hidden), clear instructions and ideogram, overview, slide show, No traditional, abstract features like a device or buttons, slides, or other confusing items.

The administration software is used to manage all data of an \ac{IMI}-exhibition. Hence, the interface has to be more complex than that of the presentation software. It is more conventional, too. The amount of different views is low though. 
\\
Because it has to be usable by lay persons, similar tasks share a \textit{view pattern}. For instance, Settings of an exhibition and an exhibit have identical controls in an identical layout. Only the views' header and the contents of the drop-down menus vary as can be seen in Figure \ref{ui_admin_settings}. The same principle applies for any definition of positions (see Figure \ref{ui_admin_positions}). No matter if the corners of an exhibition plane, the position of an exhibit or even the user position are defined, the view is always the same.
\\
The interface is designed for task-driven use. This means, that an administrative user probably will have a particular reason to use this software. Such a task includes changing a particular property. Hence, the \ac{GUI} offers the user to navigate to this property step by step. Therefore, a hierarchical structure is used, which results in less control elements in one view and thus more clarity. There is less choice but a little more actions. Given the example, an exhibit should get an additional image. As shown in Figures \ref{fig:ui_admin_scenario}a through d, a user loads the exhibition, chooses the exhibit from the drop-down menu, chooses ''New Image'' from the drop-down menu and copies with a button press. After that, a dialog opens and the user can choose the image to be added. Finally, the image appears in the images' drop-down menu. From this view, all other properties of the exhibit can be set. By pressing ''Back'', the \ac{GUI} goes back to the exhibition's view and the next task can be attended.
\\
Every time anything is changed, it is immediately saved. There are no undo- or redo-buttons. They are not necessary, because the \ac{GUI} always shows the current state of all parameters and they can be changed with no more effort than using such a button. Moreover, more control elements could cause confusion. This is why only controls that are required at a particular stage are visible. Nevertheless, there is always the option to go back to the higher level menu.
%Administration -> Shneiderman + K.I.S.S.: a little experience is needed, One thing at a time, no Drop-down menus, clear instructions and ideograms, implicit saving, calibrations abort-able

%-----------------------------------------------------------------------------

\section{Technical Principles}
\label{installation_tech}

There are certain technical principles, which the systems performance relies on. They range from operations necessary for fundamental calculations to background knowledge involving a proper interface design.

%-----------------------------------------------------------------------------

\paragraph{Pointing Challenges}

Visitors can walk up to the showcase and point at exhibits. These exhibits are lying on an exhibition plane. A plane, although usually flat and only 2-dimensional, can be defined by three Points in \ac{3D} space. Thus, to define three corners of the exhibition plane, a way to define a point in \ac{3D} space was needed. Therefore, I chose the same type of input visitors would later have to use as well. Each exhibition plane's corner is defined by pointing at it from various positions. To calculate where a person is pointing at, the system needs a vector. A vector can be determined by two points in \ac{3D} space. One defines the start and the other the direction.

After observing how people point, the elbow was chosen to be the start of the pointing vector. The pointing direction would be determined by the hand\footnote{NiTE's skeleton tracking provides both of these joints, but does not support the tracking of fingers. More details about the implementation can be found in Chapter \ref{implementation}} as Figure \ref{fig:principle_pointing} depicts.
\\
In order to define a point in \ac{3D} space, one vector was not sufficient. But, if two or more vectors would target the same point from different angles, their intersection should yield the targeted point. Figure \ref{fig:principle_intersect2d} shows this thought. Hence, a the point of their intersection could be calculated by equating both vectors. However, this is a 3-dimensional problem. The possibility of vectors intersecting in \ac{3D} space is very low. They are mostly skew, and only their projections on a \ac{2D} plane do actually intersect. Nevertheless, there is a solution. Vectors may not intersect, but there is an area where they are closest to each other. Consequently, there is one point that is closest to both vectors. This quasi-intersection is called \textit{pedal point}. It lies in the middle of the shortest line that is perpendicular to both vectors. In case both vectors do meet, pedal point and intersection are equal. Figures \ref{fig:principle_intersect3D}a and b illustrate how the projected intersection and pedal point can vary.
%- 2D Intersection is easy: get two Vectors, set both as equal, calculate intersection
%- 3D Intersection is most probably not existent $\to$ ''skew'': calculate Pedal Points (point with shortest distance to each of the skew lines) as quasi-intersections

%-----------------------------------------------------------------------------

Users are pointing at exhibits on a flat, horizontal surface. The distance between users and exhibits can vary and so does the accuracy. The further a target is away the bigger the \textit{angular error} gets. This principle can be seen in Figure \ref{fig:principle_angular_error}. For targets that are further away, $\alpha$ gets more and more acute. Hence, small variations during the pointing process have a much bigger influence with increasing distance to the target. It is an exponential problem and can be described by the \textit{law of tangents}. 
\\
A realistic sample calculation shows how big the angular error gets. The scale is an adult user whose shoulder is constantly a = 70cm above the plane. Further, the user stands an arm's length away from the plane and the target is 1.3m away from the edge. This results in a horizontal distance of $b_{target}$ = 2m between user and target\footnote{Here, the shoulder is taken as the rotational joint for illustrative reasons. However, the principle remains the same. Further, this configuration will reappear in Chapter \ref{installation_testing.}}.

$$\tan{\alpha_{target}} = \frac{b_{target}}{a} \quad \to \quad \tan{\alpha_{target}} = \frac{700mm}{2000mm} \quad \to \quad \alpha_{target} \approx 19.3^\circ$$

The target's pointing angle $\alpha_{target}$ is approximately 19$^\circ$. Due to human error, a user's arm will move. Given this movement can alternate one degree in each direction, the pointing under- or overshoots. How much this angular error influences the precision can be seen in the sample calculation.
\begin{align*}
	\tan{\alpha_{point}} = \frac{b_{point}}{a} \quad \to \quad b_{point} &= \frac{a}{\tan{\alpha_{point}}}
	\\
	b_{short} &= \frac{a}{\tan{\alpha_{short}}} \quad \to \quad b_{short} = \frac{700mm}{\tan{(18.3^\circ)}} \approx 1892mm
	\\
	b_{long} &= \frac{a}{\tan{\alpha_{long}}} \quad \to \quad b_{long} = \frac{700mm}{\tan{(20.3^\circ)}} \approx 2116mm
\end{align*}
$$\Rightarrow \quad \delta_{short} \approx 108mm \quad and \quad \delta_{long} \approx 116mm$$
This sample calculation shows that a variation of $\pm$1$^\circ$ can cause an error of nearly 120mm in pointing direction. This led me to the conclusion that, if a person can not point with pin point accuracy, the system does not have to be calibrated to this accuracy. We would need to define a \textit{sufficient accuracy} though. Hence, a target would have to have the majority of sample positions within a radius of 10cm. This is about the size of a football, with the intended point in its center.

Yet another conclusion was to not calibrate each position to its correct position in relation to the sensor and measure if users come close to them. It is counter-intuitive to force a user to re-adjust to hit those positions. Instead, I chose to use the corresponding positions that were defined by users themselves. To hit a target, users do not need its accurate position in relation to the camera, they need the accurate position of where everybody else is pointing at. Hence, all positions are \textit{virtual representations} of the real positions. Relations do not have to be correct, as long as the majority of users hit their designated target.

%-----------------------------------------------------------------------------

\paragraph{Readability} Any user should be able to read the instructional and explanatory texts displayed by the installation. Therefore, the font size and the time a text is shown had to be taken in consideration.
\\
The minimal visual angle at maximum contrast is one minute of angle or 0.017$^\circ$~\cite{SehwinkelLesbarkeit}. This only describes the ability to differentiate between a white and black dot. Readability involves whole letters and therefore needs a bigger visual angle. For people with normal or vision this angle is at five minutes of angle of 0.083$^\circ$ [ibid.]. This value is only applies for people with normal vision, which means they are able to recognize a single letter 20 feet away. People with normal eyesight have 20/20 vision, because, they can do the same. If people have 20/10 vision, they need to be 10 feet away to recognize the particular letter. A vision of 20/15 is more realistic for an average population [ibid.]. This principle can be applied to the HaÃŸleben-installation as well. Given a distance from eye to letter of $b_{20/20}$ = 2.8m and the visual angle $\alpha_{20/20}$ = 0.083$^\circ$, the height of a recognizable letter would be $a_{20/20}$ and calculated as follows:
\begin{align*}
	\tan{\alpha_{20/20}} = \frac{a_{20/20}}{b} \quad \to \quad a_{20/20} &= b \cdot \tan{\alpha_{20/20}}
	\\
	a_{20/20} &= 2800mm \cdot \tan{(0.083^\circ)} \approx 4.1mm
\end{align*}
Those 4mm are equivalent to a font size of 11.5. A text of this size is unreadable for the majority of the population though. Hence, I considered people could have bad eyesight or no glasses at hand and recalculated the font size for 20/5 vision. Since the tangent is not linear, the corresponding visual angle $\alpha_{20/5}$ could not be quadrupled. It had to be calculated according to the definition mentioned above.
$$with \ b_{20/5} = 2800mm \div 4 = 600mm$$
$$\tan{\alpha_{20/5}} = \frac{a_{20/20}}{b_{20/5}} \quad \to \quad \tan{\alpha_{20/5}} = \frac{4.1mm}{600mm} \quad \to \quad \alpha_{20/5} \approx 0.343^\circ$$
This visual angle can be used to calculate the correct size of a letter under 20/5 conditions.
$$a_{20/5} &= 2800mm \cdot \tan{(0.343^\circ)} \approx 16.8mm$$

\textbf{Font Size, Reading Speed -> Slide Times}
\\
- Sehwinkel und Lesbarkeit~\cite{SehwinkelLesbarkeit}
\\
- Lesegeschwindigkeit~\cite{Lesegeschwindigkeit}
\\
- Wortlaenge~\cite{Wortlaenge}

%-----------------------------------------------------------------------------

\section{Iterative Development}
\label{installation_testing}

Before anything could be installed or evaluated, a reliable system had to be developed. Therefore, I researched suitable environments for an extensible system. Because most \ac{SDK}s for PrimeSense's hardware are implemented in C++ or C$\#$ and Gadgeteer uses Microsoft's .NET framework and C$\#$, the final system should be implemented in C$\#$. Thus, an \ac{SDK} written in C$\#$ was to be found. After having tried several open source frameworks, the \ac{FUBI} developed at UniversitÃ¤t Augsburg proved to fit our needs best. \ac{FUBI} came with a C$\#$-wrapper, which incorporated all functionality of OpenNI and NiTE that was necessary to achieve our goals. Moreover, its leading developer, \textit{Dipl.-Inf. Felix Kistler}, kindly explained how to incorporate the new approach to \ac{FUBI}.  

\paragraph{Annotations}

\begin{itemize}
	\item Test of pointing accuracy
	\begin{enumerate}
		\item One centered Point I
		\begin{itemize}
			\item Only Pointing
			\item \textit{Images and sketches}
			\item \textit{Data and Statistics}
			\item results and conclusion
			\item See appendix
		\end{itemize}
		\item One centered Point II
		\begin{itemize}
			\item Pointing, Aiming and Combined
			\item \textit{Images and sketches}
			\item \textit{Data and Statistics}
			\item results and conlusion
			\item See appendix
		\end{itemize}
		\item Four Points on each corner of the plane
		\begin{itemize}
			\item Classification of combined values
			\item \textit{Images and sketches}
			\item \textit{Data and Statistics}
			\item results and conlusion
			\item See appendix
		\end{itemize}
	\end{enumerate}
	\item Development of algorithms for eye-hand mismatch (elbow/hand + head/hand)
	\begin{itemize}
		\item Description of Eye-Hand Mismatch [ref]
		\item \textit{Sketches of classification}
	\end{itemize}
	\item Test of algorithm's accuracy
	\begin{itemize}
		\item Target = '90 percent of all values within a 10cm radius of mean value'
		\item Differentiation between real and virtual point
		\item Necessity of 1:1-mapping of real and virtual point
	\end{itemize}
\end{itemize}

%-----------------------------------------------------------------------------

%\paragraph{Annotations}
%
%\begin{itemize}
	%\item Current State
	%\begin{itemize}
		%\item Comparing Lab- and Summaery-setup
		%\item Documentation of system's installation
	%\end{itemize}
%\end{itemize}

%-----------------------------------------------------------------------------

\section{Development Setups}
\label{setup_development}

Three installations were build. One lab-setup for development, one makeshift setup was placed in the faculties lobby, and the final one was installed inside the showcase in Museum fÃ¼r Ur- und FrÃ¼hgeschichte ThÃ¼ringens. The various setups differed more or less in dimensions and were run with different hardwares. Early tests were conducted with the lab-setup. The lobby-setup was used for a stress-test during an open door-event at the faculty, whereas the final evaluation took place in the museum. Then, only the presentation-software was tested.

\paragraph{Lab Setup} A special lab had to be found and equipped with all necessary Hardware. The Hardware was lend to me by multiple sources of the faculty, while the museum's carpenter made a pedestal consisting of a surface and feet. The surface is made out of four 9mm-press boards. The feet seemed to unstable and thus were replaced with one desk rack for each board.

%-----------------------------------------------------------------------------

\paragraph{Lobby Setup} After some technical difficulties with the museum-setup, the first test under aggravated conditions was conducted during \textit{Summ\ae{}ry}\footnote{Summ\ae{}ry is an open door-event at the faculty of media, where all chairs present their work throughout the faculty-buildings.}. Therefore, I build a makeshift setup in the facultie's lobby. It consisted of three tables forming the exhibition plane and a bar table, on which the computer and a tripod with the sensor on top were positioned. There were three targets - a candy bar, a stack of coins, and a stack of fliers - lying on the plane (\textit{see Figure}).
\\

%-----------------------------------------------------------------------------

\section{Museum Setup}
\label{setup_museum}

\begin{itemize}
	\item Automatic boot at 8:30am [Bios]
	\item Runnging
	\item Logfiles for each \textit{Session-Event}
	\begin{itemize}
		\item Start Session: User in interaction zone (Exhibition.UserPosition +/- Threshold from SessionHandler := 250mm)
		\item New Target: User pointing at a target
		\item Target Selected: Dwelltime (Exhibition.SelectionTime := 700ms) starts slide show for selected target
		\item End Session: User leaves interaction zone
	\end{itemize}
	\item Automatic shutdown at 4:45pm [Software]
\end{itemize}